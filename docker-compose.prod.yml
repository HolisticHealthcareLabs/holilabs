# ============================================================================
# Production Docker Compose Configuration
# HARDENED FOR LATAM DEPLOYMENT - OOM Protected
# ============================================================================

version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: holi-postgres-prod
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-holi}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-holi_protocol}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    # NOTE: Not exposed directly - only accessible via pgBouncer
    expose:
      - "5432"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-holi}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - holi-network

  # PgBouncer - Connection Pooling Proxy
  pgbouncer:
    image: edoburu/pgbouncer:1.21.0
    container_name: holi-pgbouncer-prod
    environment:
      # Database connection
      DATABASE_URL: postgresql://${POSTGRES_USER:-holi}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-holi_protocol}

      # Pool configuration
      # POOL_MODE: transaction (best for web apps - connections returned after each transaction)
      POOL_MODE: transaction

      # MAX_CLIENT_CONN: Maximum client connections (total connections pgBouncer accepts)
      # Set to 2x expected concurrent users (Next.js app + admin tools)
      MAX_CLIENT_CONN: 200

      # DEFAULT_POOL_SIZE: Connections per user+database pair (actual PostgreSQL connections)
      # Keep this low to prevent PostgreSQL overload
      DEFAULT_POOL_SIZE: 20

      # MIN_POOL_SIZE: Minimum pool size (keep warm connections)
      MIN_POOL_SIZE: 5

      # RESERVE_POOL_SIZE: Emergency pool when default pool is exhausted
      RESERVE_POOL_SIZE: 5

      # MAX_DB_CONNECTIONS: Total PostgreSQL connections (hard limit)
      MAX_DB_CONNECTIONS: 25

      # Timeouts (seconds)
      SERVER_IDLE_TIMEOUT: 600    # Close idle server connections after 10 min
      SERVER_LIFETIME: 3600       # Close server connections after 1 hour
      SERVER_CONNECT_TIMEOUT: 15  # Timeout connecting to PostgreSQL
      QUERY_TIMEOUT: 30           # Max query execution time
      QUERY_WAIT_TIMEOUT: 120     # Max time to wait for connection from pool
      CLIENT_IDLE_TIMEOUT: 0      # Keep client connections open indefinitely

      # Logging
      LOG_CONNECTIONS: 0
      LOG_DISCONNECTIONS: 0
      LOG_POOLER_ERRORS: 1

      # Admin access (for monitoring)
      ADMIN_USERS: ${POSTGRES_USER:-holi}
      STATS_USERS: ${POSTGRES_USER:-holi}

    ports:
      - "6432:5432"  # Expose pgBouncer on port 6432 (PostgreSQL still on 5432 internally)

    depends_on:
      postgres:
        condition: service_healthy

    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.125'

    healthcheck:
      test: ["CMD-SHELL", "PGPASSWORD=${POSTGRES_PASSWORD} psql -h localhost -U ${POSTGRES_USER:-holi} -d pgbouncer -c 'SHOW POOLS;' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

    restart: unless-stopped
    networks:
      - holi-network

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: holi-redis-prod
    command: redis-server --requirepass ${REDIS_PASSWORD} --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.125'
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - holi-network

  # Meilisearch - Fast search engine
  meilisearch:
    image: getmeili/meilisearch:v1.5
    container_name: holi-meilisearch-prod
    environment:
      MEILI_MASTER_KEY: ${MEILI_MASTER_KEY}
      MEILI_ENV: production
      MEILI_NO_ANALYTICS: "true"
      MEILI_HTTP_ADDR: 0.0.0.0:7700
    volumes:
      - meilisearch_data:/meili_data
    ports:
      - "7700:7700"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--spider", "http://localhost:7700/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - holi-network

  # Next.js Web Application (OOM PROTECTED)
  web:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
      args:
        - NODE_ENV=production
    container_name: holi-web-prod
    environment:
      # Node
      NODE_ENV: production
      PORT: 3000
      NODE_OPTIONS: "--max-old-space-size=512"

      # Database (via pgBouncer for connection pooling)
      DATABASE_URL: postgresql://${POSTGRES_USER:-holi}:${POSTGRES_PASSWORD}@pgbouncer:5432/${POSTGRES_DB:-holi_protocol}?schema=public&pgbouncer=true

      # Redis
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379

      # NextAuth
      NEXTAUTH_URL: ${NEXTAUTH_URL}
      NEXTAUTH_SECRET: ${NEXTAUTH_SECRET}
      JWT_SECRET: ${JWT_SECRET}

      # AI Services
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      DEEPGRAM_API_KEY: ${DEEPGRAM_API_KEY}

      # Email
      RESEND_API_KEY: ${RESEND_API_KEY}

      # SMS & WhatsApp
      TWILIO_ACCOUNT_SID: ${TWILIO_ACCOUNT_SID}
      TWILIO_AUTH_TOKEN: ${TWILIO_AUTH_TOKEN}
      TWILIO_PHONE_NUMBER: ${TWILIO_PHONE_NUMBER}
      TWILIO_WHATSAPP_NUMBER: ${TWILIO_WHATSAPP_NUMBER}

      # Meilisearch
      MEILI_HOST: http://meilisearch:7700
      MEILI_MASTER_KEY: ${MEILI_MASTER_KEY}

      # Presidio
      PRESIDIO_ANALYZER_URL: http://presidio-analyzer:5001
      PRESIDIO_ANONYMIZER_URL: http://presidio-anonymizer:5002
      PRESIDIO_TIMEOUT_MS: "8000"

      # Feature Flags
      ENABLE_BLOCKCHAIN: ${ENABLE_BLOCKCHAIN:-false}
      ENABLE_IPFS: ${ENABLE_IPFS:-false}

      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-info}

    ports:
      - "3000:3000"
    depends_on:
      pgbouncer:
        condition: service_healthy
      redis:
        condition: service_healthy
      meilisearch:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 768M
          cpus: '1.5'
        reservations:
          memory: 512M
          cpus: '0.5'
    # CRITICAL: Protect from OOM killer (lowest score = last to be killed)
    oom_score_adj: -500
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - holi-network

  # Presidio Analyzer (HIGH OOM PRIORITY - KILL THIS FIRST)
  presidio-analyzer:
    image: mcr.microsoft.com/presidio-analyzer:latest
    container_name: holi-presidio-analyzer
    environment:
      - LOG_LEVEL=INFO
      - GRPC_PORT=5001
      - NLP_ENGINE_NAME=spacy
      - MODELS_CACHE_DIR=/app/models
      - NLP_WORKERS=1
      - DEFAULT_LANGUAGE=es
      - SUPPORTED_LANGUAGES=es,pt,en
    volumes:
      - presidio-models:/app/models
    ports:
      - "5001:5001"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 768M
          cpus: '0.5'
    # CRITICAL: Kill Presidio FIRST to protect Next.js (high score = first to die)
    oom_score_adj: 500
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - holi-network
    labels:
      - "coolify.managed=true"
      - "coolify.service=presidio-analyzer"
      - "coolify.port=5001"

  # Presidio Anonymizer
  presidio-anonymizer:
    image: mcr.microsoft.com/presidio-anonymizer:latest
    container_name: holi-presidio-anonymizer
    environment:
      - LOG_LEVEL=INFO
      - GRPC_PORT=5002
      - DEFAULT_ANONYMIZER=replace
      - CACHE_ENABLED=true
    ports:
      - "5002:5002"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - holi-network
    labels:
      - "coolify.managed=true"
      - "coolify.service=presidio-anonymizer"
      - "coolify.port=5002"

  # Redis for Presidio Cache
  presidio-redis:
    image: redis:7-alpine
    container_name: holi-presidio-redis
    command: redis-server --maxmemory 128mb --maxmemory-policy allkeys-lru
    volumes:
      - presidio-redis-data:/data
    ports:
      - "6380:6379"
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - holi-network

  # Nginx Reverse Proxy (optional)
  nginx:
    image: nginx:alpine
    container_name: holi-nginx-prod
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./nginx/logs:/var/log/nginx
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - web
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--spider", "http://localhost/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - holi-network

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  meilisearch_data:
    driver: local
  presidio-models:
    driver: local
  presidio-redis-data:
    driver: local

networks:
  holi-network:
    driver: bridge
    name: holi-network

# ============================================================================
# RESOURCE SUMMARY (4GB Droplet):
# - PostgreSQL: 512MB
# - PgBouncer: 128MB (connection pooling proxy)
# - Redis: 256MB
# - Meilisearch: 512MB
# - Next.js: 768MB (OOM protected, oom_score=-500)
# - Presidio Analyzer: 1GB (Will be killed first, oom_score=500)
# - Presidio Anonymizer: 512MB
# - Presidio Redis: 256MB
# - Nginx: 128MB
# TOTAL: ~3.6GB + 400MB system overhead = Fits in 4GB with headroom
#
# NOTE: Application connects to pgBouncer (port 6432) instead of PostgreSQL
#       directly (port 5432). This allows 200 client connections while only
#       using 20-25 actual PostgreSQL connections, preventing connection
#       exhaustion under load.
# ============================================================================
